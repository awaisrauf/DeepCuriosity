---
title: ICCV 2019 at Seoul
published: False
---
With the fast pace advancements in machine learning community, conferencces have becomme frontier of the field. Unlike many other sciences, most of the research in machine learning is presented in these conferences. Interntaional Conference on Computer Vision (ICCV) is one such venue which is considered to be top venue in both deep leanring and computer vision. [ICCV19](http://iccv2019.thecvf.com/) was held in Seoul last week. I had honour to attend it, thanks to my professor [Dr. Sung-Ho Bae]. I had such a great expirence and I am tempted to share it here. 


![enter image description here](https://lh3.googleusercontent.com/HCCrMOiaoJYluGJAPLUCmqytmUsUjILqs_e13KXd6QXwWbfYViOs8npGFRbObBnXNcNB35aP6PI)

ICCV is divided into two parts; main conference and workshops and tutorials. First two and last day was designated for workshops-tutorials.


### Workshops and Tutorials

 On the first day, I attended two two different talks from [Workshop on statistical deep learning]() and [Subspace leanring](). The first [talk](http://www.sdlcv-workshop.com/slides/talk_WuKorea1.pdf) was on Matrix and vector representation in Neuroscience by [Prof. Ying Nian Wu](http://www.stat.ucla.edu/~ywu/). 
>The main theme of the talk was around the [1]. Mamilians cells have neurons called place and grid cells. Place cells fire only when we encounter a specific place while grid makes certian hexagonal firing patterns. These neurons are responsible for path planning, motion and even called our GPS becuase of their assistance in nvaigation even in the absence of visual clues. This paper showed that self-location of agent can be reprsented by a vector and self-displacement with a mtrix in higher dimensions in such a way that path planning, displacement and many other activites can be represented as transformations on these cells. The even showed that hexagonal patterns emerged by these cells can also be recovered via these representation.

I also met with Dr. Ping Luo after this talk. [Dr. Ping Luo](http://luoping.me/) is assistant professor at Univeristy of HongKong and ex. Research Director of SenseTime. He has a very extensive research profile and fortunately, his lab has many publications related to BatchNorm on which I was working for last two semester. Earlier, I emalied him for possible research internship oppourtunity and he was agreed to meet me. It was really talking with him about the his research. 
 
The second talk, I have attended was on Tensor  factorization for robust learning by Jean Kossaifi which was based on [2].
> Talk was around the theme of use of tensor decomposition in deep neural networks which decreses computation complexity and increases robustness. The showed representing whole neural network with a higher dimensional tensor makes it light weight, more robust while maintaining accuracy, Jean Kossaifi is also author of famous python library for this purpose called [Tensorly](http://tensorly.org/)

In second half of this day, I attended tutorial on [interpretation of Deep learning]([https://interpretablevision.github.io/](https://interpretablevision.github.io/)) which was jam-packed similar to its predecessor in CVPR. 

![](https://interpretablevision.github.io/figures/iccv19_meeting.jpg)


The first talk was on [Understanding models via visualizations and attribution](https://interpretablevision.github.io/slide/iccv19_vedaldi_slide.pdf) Andrea Veladi. (following images are taken from these slides)
> Despite impressive performance, deep neural networks are blackboxes and understanding is very important. We can divide understanding them into three parts; what does a network do, how does it do it and how does it learn. This talk was on seond aspect i.e. how does a network do a specific task. And this talk was revolving around two centeral questions; what do an intermediate layer of DNN know about input and what does a neural network see in the input image.   
> *First Question*:  What a model knows of input image $x$ at an intermediate layer $y$,  as shown in the figure,![](https://lh3.googleusercontent.com/E1OAdYaxnHR9VSJE22DQ_Hrcn0zDZaljyMgSzQy8R4tefreBd6cooJU4YQFVr4DZM72FZlx7m-0)
>  One way to answer this question is to projecrt $y$ on input space and get $x_0$. This can be done by simple reconstrction i.e. start from a random initilization of $x_0$ and minizie $||\Phi(x) - \Phi(x_0)||$ and get so called pre-image or $x_0$. This reconstruction is done without any training data. There are a number of ways to reconstruct this pre-image such as [3,4,5]. Intrestingly, reconstruction from any layer show a striking similarity to input. Even last fully connected layer does have some similarity with input image.  ![enter image description here](https://lh3.googleusercontent.com/HdTi6nI7izhpCNIGuVPHJ0UDVetch85RkAFxLY25WSZG1nRktAfD1hZERQme0ufDhn8u9bqFiPs)
>  Deep image prior [4] had previously showed a very intresting result that we can fit a neural network with a single image and can perform many tasks. If we fit a neural network with a single image and then ask above mentioned question from this neural network, it still gives a very clear reconstruction of input image. This is interesting because neural network is not trained on any kind of data. For an intresting discussion on this topic, see [this](https://distill.pub/2018/building-blocks/) 
>  *Question 2* What do a neural network see in input image.  One stragiht forward way to answer this question by visulize gradients of a network w.r.t to input image [6,7,8]. 
![enter image description here](https://lh3.googleusercontent.com/LVVJMwmgJnmkFjsyoqODc0EzRkdMORCKH3xQ_HXjO0kc-i3xvPRcHstapyCF-NDv1OiSZqySrso)
These visulization do give some hints but are not very clear. This problem can be solved by smoothing gradients[9]. But, all of these visulizations method have problem of channel specificity i.e. changing output class does not change visulization. 
![](https://lh3.googleusercontent.com/Vjfu-y96okYLdlUTAGTZNEbxA7RX00KpdAald1HOdVE1jegnpRecd7DUO3rKm9qoTZts82SXyII)
This problem can be solved with CAM and GradCAM [9,10] where we use an intermediate layer to see prominent parts of input image. 
One question that is worth asking is what does these visulization means? Andrew argued that gradient can be think of as a local linear approximation of the model so we can think of gradeint based visulizations as senstivity analysis. This is where new work on pertubation analysis [12] comes along which is to understand neural network via intresting pertubations i.e. by injecting noise, rotation, erasing parts of input etc. and see network behaviour.  
![](https://lh3.googleusercontent.com/xwGC_5cqRysKmcU1p6odc0cxizzh58T9RakYICITzbJZQmOOwVUTnE-TIScMdbTjb01a5T1_DqI)
This analysis gives a number of intresting results such as foreground might be sufficient but eliminating background can change the network in negative way, and many more.

Second talk was deliverd by [Bolei Zhou](http://bzhou.ie.cuhk.edu.hk/) on understanding latent semantics. In last some years, GANs are becoming better at producing realistic and diverse images. This talk was on understading this generation process. The talk revolved around three things; understanding role of neurons, semantics in latent space and biases and limitatoins. 
First question was on the role of individual neurons in image generation. GAN dissection paper[13] solved this riddle by turning on-off individual neurons and then using semantic segmentation to find effect of it in generated images. Intrestingly, they found that different neurons are responsible for different kind of objects and scenes in a GAN. For instance, one can find a neuron which is responsible to generate trees in a scene and then by turing this neuron off or on, one can include/exclude these objects. Following figure shows effect of different units. You can try a demo of it [here](ganpaint.io).
![](https://lh3.googleusercontent.com/Q4kHb4eT0nEajX3t9l7xiGHbfzy57sLZMXkLUoTTkMnEqqEbZ8cSJiFBP0-crDD2XEzjssBx14Y)  
GANs generate image given a random vector from latent space. A random walk in this latent space changes the generated image in a meaningful way. This shows relationship between this vector and semantics of generated image. In an onging work, Bolei Zhou et all. showed that it is infact possible to classify latent space according to attributes. For example, one can change find boundry of latent space to turn indoor lights on/off. 
Third question was, can we use GAN to reconstruct an image i.e. given an image $x$, can we find its latent space $z$ such that GAN can reproduce original image $x$. While GAN can reconstruct images, generated image turns out to be different than original as shown in figure.
![enter image description here](https://lh3.googleusercontent.com/uOLADqFIBAWAvXTEXYCdFNq-AH0bvmXOvcBZOzGS28eVrrmPDYdAI6OKxg-tnmY0wMxKBqBBazs)
As shown in paper [14], GAN don't like many things such as people, vehicle, sign etc. Similarly, given an Asian face, GANs tend to make it white. 
After the talk, I was able to ask few questions from Zhou. One question that came to my mind from Asian face to White face phenomenon is what is root of the problem of issues in reconstruction; data distribution or GAN themselves? To me it seems like a data rather than GANs problem. 

The third and last talk of the session and the day [Explaining deep learning for identifying structures and biases in computer vision](https://interpretablevision.github.io/slide/iccv19_binder_slide.pdf) by [Prof. Alexander Binder](https://scholar.google.de/citations?user=5B8CTlEAAAAJ&hl=de). The talk was around the concept  of layer-wise relevance propogation (LRP) to interpret neural networks. In this method, score for each dimension of input is calculted and used as explnation. This method successfully found game startegies learn by  DNN to play attari breakout. He also discussed different ways to find biases in datasets. 

 On the second day of the workshops and tutorials, I decided to take [Neural Archtecures](https://neuralarchitects.org/) workshop arranged by VGG group in Oxford University.  The first talk was [Neural Archtecure and Beyond](https://neuralarchitects.org/slides/zoph-slides.pdf) by [Barret Zoph](http://barretzoph.github.io/). This talk gave a holistic view of progress in archtecures of neural networks. They divided AI into four periods; good old fashioned where handcrafted feaures were used without any learning, second when handcrafted features were used to fed a learning algorithm, third recent deep learning wave where most of the handcrafting thing is done at the algorihtmic level i.e. change of archtecure etc. and fourth learn to learn or neural archtecure search. 
![](https://lh3.googleusercontent.com/iH-WJ-K9LFmdx9un13pXTigU_c-5iHifEsFOHvrSq1pwFUAYEym2TDjwqLrUT7Y9cQ8woxrWUBE)
While it can be argued that most of recent benefits in Deep Learning came from improvements in archtecure (AlexNet [16], VGG [17], ResNet[18], DenseNet[19], SENet [20] etc. ), this is also one of the most difficult, time consuming and almost more of an artistic kind of a job. It requires a lot of human effort, inutition and experimentation. So this is an ideal task for machine learning i.e. make a neural network to learn archtecure of neural network. This was specifically done by [21, 22]. Specifically, these papers introduced a reinforcement leanring oriented way to find best archtecures. In this settings, a controller proposes an archtecure from a search space, this proposed network is trained on a subset of dataset, then this trained classifier's accuracy and other relevant parameters are fed to controller as reward. This way, controller learned to output best performing archtecure.  
![704601374](https://lh3.googleusercontent.com/Ys4Dwm8OCUmntwFLt6pjjXTN484ckM939msn8l4HDEISShUXqpHxFP-R2YqdOp2k-SqES5qe2mk)
Mostly, an RNN controller is used while search space consisits of different architecure elements such as convolution layer, BN lyaers, connections etc. Using this way, an effecient archtecure can be found for almost any field. It has been shown to work in classification, segmentation, video processing, machine translation and many other tasks as well. 

It is also intresting to inspect decisions made by these controllers. While most of the decisions are not very understandable, a few do give intresting insights. For instance, in many of the proposed archtecures, initial layers uses more convolution layer than later layers. 

Plateform aware NAS [23]






### References 
[1] Gao, Ruiqi, et al. "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion." (2018).

[2] Kossaifi, Jean, et al. "T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order Tensor." _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_. 2019.

[3] Mahendran, Aravindh, and Andrea Vedaldi. "Understanding deep image representations by inverting them." _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2015.

[4] Ulyanov, Dmitry, Andrea Vedaldi, and Victor Lempitsky. "Deep image prior." _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_. 2018.

[5] Nguyen, Anh, et al. "Plug & play generative networks: Conditional iterative generation of images in latent space." _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_. 2017.

[6] Zeiler, Matthew D., and Rob Fergus. "Visualizing and understanding convolutional networks." _European conference on computer vision_. Springer, Cham, 2014.

[7] Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. "Deep inside convolutional networks: Visualising image classification models and saliency maps." _arXiv preprint arXiv:1312.6034_ (2013).

[8] Springenberg, Jost Tobias, et al. "Striving for simplicity: The all convolutional net." _arXiv preprint arXiv:1412.6806_ (2014).

[9] Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. "Axiomatic attribution for deep networks." _Proceedings of the 34th International Conference on Machine Learning-Volume 70_. JMLR. org, 2017.

[10] Zhou, Bolei, et al. "Learning deep features for discriminative localization." _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2016.

[11] Selvaraju, Ramprasaath R., et al. "Grad-cam: Visual explanations from deep networks via gradient-based localization." _Proceedings of the IEEE International Conference on Computer Vision_. 2017.

[12] Fong, Ruth, Mandela Patrick, and Andrea Vedaldi. "Understanding Deep Networks via Extremal Perturbations and Smooth Masks." _Proceedings of the IEEE International Conference on Computer Vision_. 2019.

[13] Bau, David, et al. "Gan dissection: Visualizing and understanding generative adversarial networks." _arXiv preprint arXiv:1811.10597_ (2018).

[14] Bau, David, et al. "Seeing What a GAN Cannot Generate." _Proceedings of the IEEE International Conference on Computer Vision_. 2019.

[15] Montavon, Grégoire, et al. "Layer-wise relevance propagation: an overview." _Explainable AI: Interpreting, Explaining and Visualizing Deep Learning_. Springer, Cham, 2019. 193-209.

[16] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." _Advances in neural information processing systems_. 2012.

[17] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." _arXiv preprint arXiv:1409.1556_ (2014).

[18] He, Kaiming, et al. "Deep residual learning for image recognition." _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2016.

[19] Huang, Gao, et al. "Densely connected convolutional networks." _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2017.

[20] Hu, Jie, Li Shen, and Gang Sun. "Squeeze-and-excitation networks." _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2018.

[21] Zoph, Barret, et al. "Learning transferable architectures for scalable image recognition." _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2018.

[22] Real, Esteban, et al. "Large-scale evolution of image classifiers." _Proceedings of the 34th International Conference on Machine Learning-Volume 70_. JMLR. org, 2017.

[23] Wu, Bichen, et al. "Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search." _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_. 2019.



