---


---

<hr>
<h2 id="title-iccv-2019-at-seoulpublished-true">title: ICCV 2019 at Seoul<br>
published: True</h2>
<p>With the fast pace advancements in machine learning community, conferencces have becomme frontier of the field. Unlike many other sciences, most of the research in machine learning is presented in these conferences. Interntaional Conference on Computer Vision (ICCV) is one such venue which is considered to be top venue in both deep leanring and computer vision. <a href="http://iccv2019.thecvf.com/">ICCV19</a> was held in Seoul last week. I had honour to attend it, thanks to my professor [Dr. Sung-Ho Bae]. I had such a great expirence and I am tempted to share it here.</p>
<p>ICCV is divided into two parts; main conference and workshops and tutorials. First two and last day was designated for workshops-tutorials.</p>
<h3 id="workshops-and-tutorials">Workshops and Tutorials</h3>
<p>On the first day, I attended two two different talks from <a href="">Workshop on statistical deep learning</a> and <a href="">Subspace leanring</a>. The first <a href="http://www.sdlcv-workshop.com/slides/talk_WuKorea1.pdf">talk</a> was on Matrix and vector representation in Neuroscience by <a href="http://www.stat.ucla.edu/~ywu/">Prof. Ying Nian Wu</a>.</p>
<blockquote>
<p>The main theme of the talk was around the [1]. Mamilians cells have neurons called place and grid cells. Place cells fire only when we encounter a specific place while grid makes certian hexagonal firing patterns. These neurons are responsible for path planning, motion and even called our GPS becuase of their assistance in nvaigation even in the absence of visual clues. This paper showed that self-location of agent can be reprsented by a vector and self-displacement with a mtrix in higher dimensions in such a way that path planning, displacement and many other activites can be represented as transformations on these cells. The even showed that hexagonal patterns emerged by these cells can also be recovered via these representation.</p>
</blockquote>
<p>The second talk, I have attended was on Tensor  factorization for robust learning by Jean Kossaifi which was based on [2].</p>
<blockquote>
<p>Talk was around the theme of use of tensor decomposition in deep neural networks which decreses computation complexity and increases robustness. The showed representing whole neural network with a higher dimensional tensor makes it light weight, more robust while maintaining accuracy, Jean Kossaifi is also author of famous python library for this purpose called <a href="http://tensorly.org/">Tensorly</a></p>
</blockquote>
<p>In second half of this day, I attended tutorial on <a href="%5Bhttps://interpretablevision.github.io/%5D(https://interpretablevision.github.io/)">interpretation of Deep learning</a> which was jam-packed similar to its predecessor in CVPR.</p>
<p><img src="https://interpretablevision.github.io/figures/iccv19_meeting.jpg" alt=""></p>
<p>The first talk was on <a href="https://interpretablevision.github.io/slide/iccv19_vedaldi_slide.pdf">Understanding models via visualizations and attribution</a> Andrea Veladi. (following images are taken from these slides)</p>
<blockquote>
<p>Despite impressive performance, deep neural networks are blackboxes and understanding is very important. We can divide understanding them into three parts; what does a network do, how does it do it and how does it learn. This talk was on seond aspect i.e. how does a network do a specific task. And this talk was revolving around two centeral questions; what do an intermediate layer of DNN know about input and what does a neural network see in the input image.<br>
<em>First Question</em>:  What a model knows of input image <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">x</span></span></span></span></span> at an intermediate layer <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span></span></span></span></span>,  as shown in the figure,<img src="https://lh3.googleusercontent.com/E1OAdYaxnHR9VSJE22DQ_Hrcn0zDZaljyMgSzQy8R4tefreBd6cooJU4YQFVr4DZM72FZlx7m-0" alt=""><br>
One way to answer this question is to projecrt <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span></span></span></span></span> on input space and get <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.58056em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>. This can be done by simple reconstrction i.e. start from a random initilization of <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.58056em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> and minizie <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Φ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi mathvariant="normal">Φ</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">||\Phi(x) - \Phi(x_0)||</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">∣</span><span class="mord">∣</span><span class="mord">Φ</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">Φ</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">∣</span><span class="mord">∣</span></span></span></span></span> and get so called pre-image or <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.58056em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>. This reconstruction is done without any training data. There are a number of ways to reconstruct this pre-image such as [3,4,5]. Intrestingly, reconstruction from any layer show a striking similarity to input. Even last fully connected layer does have some similarity with input image.  <img src="https://lh3.googleusercontent.com/HdTi6nI7izhpCNIGuVPHJ0UDVetch85RkAFxLY25WSZG1nRktAfD1hZERQme0ufDhn8u9bqFiPs" alt="enter image description here"><br>
Deep image prior [4] had previously showed a very intresting result that we can fit a neural network with a single image and can perform many tasks. If we fit a neural network with a single image and then ask above mentioned question from this neural network, it still gives a very clear reconstruction of input image. This is interesting because neural network is not trained on any kind of data. For an intresting discussion on this topic, see <a href="https://distill.pub/2018/building-blocks/">this</a><br>
<em>Question 2</em> What do a neural network see in input image.  One stragiht forward way to answer this question by visulize gradients of a network w.r.t to input image [6,7,8].<br>
<img src="https://lh3.googleusercontent.com/LVVJMwmgJnmkFjsyoqODc0EzRkdMORCKH3xQ_HXjO0kc-i3xvPRcHstapyCF-NDv1OiSZqySrso" alt="enter image description here"><br>
These visulization do give some hints but are not very clear. This problem can be solved by smoothing gradients[9]. But, all of these visulizations method have problem of channel specificity i.e. changing output class does not change visulization.<br>
<img src="https://lh3.googleusercontent.com/Vjfu-y96okYLdlUTAGTZNEbxA7RX00KpdAald1HOdVE1jegnpRecd7DUO3rKm9qoTZts82SXyII" alt=""><br>
This problem can be solved with CAM and GradCAM [9,10] where we use an intermediate layer to see prominent parts of input image.<br>
One question that is worth asking is what does these visulization means? Andrew argued that gradient can be think of as a local linear approximation of the model so we can think of gradeint based visulizations as senstivity analysis. This is where new work on pertubation analysis [12] comes along which is to understand neural network via intresting pertubations i.e. by injecting noise, rotation, erasing parts of input etc. and see network behaviour.<br>
<img src="https://lh3.googleusercontent.com/xwGC_5cqRysKmcU1p6odc0cxizzh58T9RakYICITzbJZQmOOwVUTnE-TIScMdbTjb01a5T1_DqI" alt=""><br>
This analysis gives a number of intresting results such as foreground might be sufficient but eliminating background can change the network in negative way, and many more.</p>
</blockquote>
<p>Second talk was deliverd by <a href="http://bzhou.ie.cuhk.edu.hk/">Bolei Zhou</a> on understanding latent semantics. In last some years, GANs are becoming better at producing realistic and diverse images. This talk was on understading this generation process. The talk revolved around three things; understanding role of neurons, semantics in latent space and biases and limitatoins.<br>
First question was on the role of individual neurons in image generation. GAN dissection [13] which tried to assign each neuron an individual</p>
<h3 id="references">References</h3>
<p>[1] Gao, Ruiqi, et al. “Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion.” (2018).</p>
<p>[2] Kossaifi, Jean, et al. “T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order Tensor.” <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 2019.</p>
<p>[3] Mahendran, Aravindh, and Andrea Vedaldi. “Understanding deep image representations by inverting them.” <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2015.</p>
<p>[4] Ulyanov, Dmitry, Andrea Vedaldi, and Victor Lempitsky. “Deep image prior.” <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 2018.</p>
<p>[5] Nguyen, Anh, et al. “Plug &amp; play generative networks: Conditional iterative generation of images in latent space.” <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 2017.</p>
<p>[6] Zeiler, Matthew D., and Rob Fergus. “Visualizing and understanding convolutional networks.” <em>European conference on computer vision</em>. Springer, Cham, 2014.</p>
<p>[7] Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. “Deep inside convolutional networks: Visualising image classification models and saliency maps.” <em>arXiv preprint arXiv:1312.6034</em> (2013).</p>
<p>[8] Springenberg, Jost Tobias, et al. “Striving for simplicity: The all convolutional net.” <em>arXiv preprint arXiv:1412.6806</em> (2014).</p>
<p>[9] Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. “Axiomatic attribution for deep networks.” <em>Proceedings of the 34th International Conference on Machine Learning-Volume 70</em>. JMLR. org, 2017.</p>
<p>[10] Zhou, Bolei, et al. “Learning deep features for discriminative localization.” <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2016.</p>
<p>[11] Selvaraju, Ramprasaath R., et al. “Grad-cam: Visual explanations from deep networks via gradient-based localization.” <em>Proceedings of the IEEE International Conference on Computer Vision</em>. 2017.</p>
<p>[12] Fong, Ruth, Mandela Patrick, and Andrea Vedaldi. “Understanding Deep Networks via Extremal Perturbations and Smooth Masks.” <em>Proceedings of the IEEE International Conference on Computer Vision</em>. 2019.</p>
<p>[13] Bau, David, et al. “Gan dissection: Visualizing and understanding generative adversarial networks.” <em>arXiv preprint arXiv:1811.10597</em> (2018).</p>

